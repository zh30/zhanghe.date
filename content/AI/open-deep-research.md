---
title: 开源的“深度研究”
date: 2025-02-05
---
话说昨天 OpenAI 放了个大招，搞了个叫“深度研究 (Deep Research)”的玩意儿，能自动上网找资料，总结内容，还能回答问题，简直厉害到飞起！

他们自己发的博客里说，这玩意儿在 GAIA 这个 AI 助手 benchmark 上表现巨好，平均一次性就能答对 67% 的问题，那些特别难的“Level 3”问题（得推理好几步，还得用工具）也能搞定 47.6%。GAIA 到底是个啥？后面再细说。

“深度研究”这玩意儿其实就是个 LLM (大型语言模型，你可以选 OpenAI 提供的各种模型，比如 4o, o1, o3 啥的) 加上一个内部的“智能体框架 (agentic framework)”，这框架指挥 LLM 去用各种工具，比如网页搜索，然后把动作一步一步组织起来。

现在开源的 LLM 也挺牛逼的，比如最近的 DeepSeek R1 模型。但是 OpenAI 对“深度研究”底层的智能体框架，那是啥也没说…

所以，Hugging Face 就决定花 24 小时，也山寨一个出来，顺便把需要的框架开源了！

时间紧任务重，开搞！ ⏱️

## 啥是智能体框架？为啥它很重要？

智能体框架，其实就是在 LLM 之上加一层，让 LLM 可以执行各种动作 (比如上网或者读 PDF 文件)，还能把操作按步骤组织好。

想快速了解智能体？可以看看 Andrew Ng 的采访，或者看看 Hugging Face smolagents 库的介绍博客。想深入了解？可以报名 Hugging Face 的智能体课程： [链接](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=9ed45a3ef6).

现在大家玩聊天机器人，都知道 LLM 有多厉害。但很多人还不知道，把 LLM 集成到智能体系统里，能让它们拥有真正的超能力！

最近有个例子，对比了一些顶尖 LLM 在有无智能体框架 (这里用的是简单的 smolagents 库) 下的表现 – 用了智能体框架，性能提升了 60 个点！

OpenAI 在博客里也说了，“深度研究”在知识密集型的“人类终极考试 (Humanity’s Last Exam)”benchmark 上，比单独的 LLM 表现好太多了。

所以，如果我们把现在最牛逼的 LLM 放到智能体框架里，搞一个开源的“深度研究”，会发生啥？

先说清楚：我们会用 GAIA challenge 来 benchmark，但这只是个开始。“深度研究”是个大工程，完全山寨出来需要时间。特别是，要达到完全一样的效果，需要更好的浏览器使用和交互，就像 OpenAI Operator 提供的那样，不能只像我们现在这样，搞个只能处理文本的网页交互。

先来了解下 GAIA 这个 challenge：

## GAIA benchmark 介绍

GAIA 可以说是最全面的智能体 benchmark 了。它的问题非常难，涉及到 LLM 系统的很多挑战。比如：

> 在 2008 年的画作《乌兹别克斯坦的刺绣》中展示的哪些水果，在 1949 年 10 月被用作“最后航程”电影的漂浮道具的远洋班轮的早餐菜单的一部分？按顺时针方向给出项目，从画作中的 12 点位置开始。使用每种水果的复数形式。

这个问题涉及很多挑战：

*   用指定的格式回答
*   用到多模态能力 (从图片里提取水果)
*   收集多个信息，有些信息还得依赖其他信息：
    *   识别图片里的水果
    *   找到哪艘远洋班轮被用作“最后航程”的漂浮道具
    *   找到那艘远洋班轮 1949 年 10 月的早餐菜单
*   按正确的顺序串联问题解决的过程

解决这个问题，既需要高层次的规划能力，也需要严格的执行，而这两点都是单独使用 LLM 的弱项。

所以，GAIA 是个测试智能体系统的绝佳选择！

在 GAIA 的公开 leaderboard 上，不用任何智能体框架，GPT-4 在验证集上的得分连 7% 都不到。而 OpenAI 的“深度研究”，在验证集上达到了 67.36% 的高分！提升了一个数量级！(虽然我们不知道他们在私有测试集上表现如何。)

看看我们能不能用开源工具做得更好！

## 打造一个开源的“深度研究”

### 用 CodeAgent

我们要做的第一个改进，是使用所谓的“代码智能体 (code agent)”。正如 Wang et al. (2024) 所说，让智能体用代码表达它的动作，有很多优点，最重要的是，代码是专门用来表达复杂动作序列的。

看下 Wang et al. 举的例子：

使用代码的好处多多：

*   代码动作比 JSON 更简洁
*   需要并行运行 4 个流，每个流包含 5 个连续动作？用 JSON，你需要生成 20 个 JSON blob，每个 blob 都在单独的步骤里；用代码，只需要 1 步。
*   论文表明，代码动作比 JSON 减少了 30% 的步骤，相当于减少了生成的 tokens 数量。由于 LLM 调用通常是智能体系统的主要成本，这意味着你的智能体系统运行成本降低了 ~30%。
*   代码可以使用通用库里的工具
*   benchmark 表现更好，原因有二：
    *   表达动作的方式更直观
    *   LLM 在训练时接触了大量的代码

我们在 agent\_reasoning\_benchmark 上的实验也证实了以上优点。

从构建 smolagents 的经验来看，还有一个显著的优点，那就是更好地处理状态：这对于多模态任务尤其有用。需要存储图像/音频/其他东西以供以后使用？没问题，只需将其作为变量分配到你的状态中，你可以在需要的步骤中重用它。使用 JSON，你必须让 LLM 在字典键中命名它，并相信 LLM 稍后会理解它仍然可以使用它。

### 打造好用的工具 🛠️

现在我们需要为智能体提供合适的工具。

1.  网页浏览器。虽然像 Operator 这样功能齐全的网页浏览器交互是达到最佳性能所必需的，但我们首先使用了一个非常简单的基于文本的网页浏览器，作为我们的第一个概念验证。你可以在[这里](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/examples/open_deep_research/scripts/text_web_browser.py)找到代码
2.  一个简单的文本检查器，能够读取一堆文本文件格式，在这里找到它。

这些工具来自微软研究院出色的 Magentic-One 智能体，感谢他们！我们没有对它们进行太多更改，因为我们的目标是以最低的复杂性获得尽可能高的性能。

以下是我们认为可以真正提高这些工具性能的改进的简短路线图（欢迎提出 PR 并做出贡献！）：

*   扩展可以读取的文件格式的数量。
*   提出更细粒度的文件处理。
*   用基于视觉的浏览器替换网页浏览器，Hugging Face 已经[开始](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/src/smolagents/vision_web_browser.py)这样做了。

## 结果 🏅

在我们的 24 小时以上的山寨冲刺中，已经看到了我们的智能体在 GAIA 上的性能稳步提高！

我们已经迅速从以前的开源框架 SoTA（Magentic-One 大约为 46%）提升到目前在验证集上的 54% 的性能。

性能的提升主要归功于让我们的智能体用代码编写他们的动作！事实上，当切换到在 JSON 中编写动作而不是代码的标准智能体时，相同设置的性能会立即降低到验证集上的平均 33%。

然而，这仅仅是个开始，还有很多事情需要改进！我们的开源工具可以做得更好，smolagents 框架也可以进行调整，我们很乐意探索更好的开源模型的性能来支持该智能体。

我们欢迎社区加入 Hugging Face，共同利用开源研究的力量来构建一个出色的开源智能体框架！这样任何人都可以使用他们最喜欢的模型，以完全本地化和定制的方式在家中运行类似 DeepResearch 的智能体！

## 社区复现

在我们致力于此并专注于 GAIA 的同时，社区中涌现了其他出色的 Deep Research 开源实现，特别是来自以下人员：

*   dzhng,
*   assafelovic,
*   nickscamara,
*   jina-ai 和
*   mshumer.

这些实现中的每一个都使用不同的库来索引数据、浏览网络和查询 LLM。在这个项目中，我们希望重现 OpenAI 提出的基准（pass@1 平均分），对切换到开源 LLM（如 DeepSeek R1）进行基准测试并记录我们的发现，使用视觉 LM，以及针对代码原生智能体对传统工具调用进行基准测试。

## 下一步最重要的事

OpenAI 的 Deep Research 可能会受到他们随 Operator 引入的出色网页浏览器的推动。

所以我们接下来要解决这个问题！在更一般的问题中：我们将构建 GUI 智能体，即“查看你的屏幕并可以直接使用鼠标和键盘操作的智能体”。如果你对这个项目感到兴奋，并希望帮助每个人通过开源访问如此酷的功能，我们很乐意得到你的贡献！

[原文链接](https://huggingface.co/blog/open-deep-research)