---
title: GUI 自动化神器 OmniParser V2 来啦！
date: 2025-02-19
tags:
  - 工具
---
## OmniParser V2：让 AI 轻松看懂你的屏幕

有没有觉得让 AI 自动操作电脑屏幕上的各种图形界面 (GUI) 超难？一般的大语言模型（LLM）要做到这一点，面临着两大难题：一是很难准确地识别出界面上可以点击的图标；二是就算看到了，也搞不懂这些元素代表什么意思，更不知道该点哪里才能执行正确的操作。

现在，OmniParser V2 横空出世，完美解决了这个问题！它就像一个“翻译器”，能把屏幕上的像素变成 LLM 能理解的结构化元素。简单来说，OmniParser V2 可以把整个屏幕“token 化”，让 LLM 知道哪里可以点，点了会发生什么，然后根据你的指令做出正确的选择。

相比于上一代版本，OmniParser V2 更加强大。它能更准确地检测到更小的可交互元素，而且速度更快！之所以这么厉害，是因为它用了更多的数据进行训练，特别是关于交互元素检测和图标功能描述的数据。而且，通过减小图标描述模型的大小，OmniParser V2 的延迟降低了 60%。更牛的是，OmniParser V2 + GPT-4o 在最新的 ScreenSpot Pro 基准测试中取得了 39.6% 的平均准确率，刷新了纪录！要知道，这个基准测试可是以高分辨率屏幕和小图标为特色，非常具有挑战性。作为对比，GPT-4o 原本的得分只有 0.8。

## OmniTool：快速体验各种 AI Agent

为了方便大家更快地试验不同的 Agent 设置，我们还开发了一个名为 OmniTool 的工具，它是一个 Docker 化的 Windows 系统，内置了一系列对于 Agent 来说必不可少的工具。

通过 OmniTool，你可以直接将 OmniParser V2 与各种顶尖的 LLM 结合使用，例如：OpenAI (4o/o1/o3-mini), DeepSeek (R1), Qwen (2.5VL) 和 Anthropic (Sonnet)。这样，就能完整地实现屏幕理解、定位、行动规划和执行等步骤。

## 风险与对策

为了符合微软的 AI 原则和负责任的 AI 实践，我们采取了一些措施来降低风险。我们使用负责任的 AI 数据来训练图标描述模型，尽量避免模型推断出图标中人物的敏感属性（例如种族、宗教等）。同时，我们建议用户只将 OmniParser V2 用于不包含有害内容的屏幕截图。

对于 OmniTool，我们使用微软的威胁建模工具进行了威胁模型分析。我们在 GitHub 仓库中提供了沙盒 Docker 容器、安全指南和示例。并且我们建议始终让人工参与到过程中，以便尽可能地降低风险。